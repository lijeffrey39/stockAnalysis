{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readPickleObject(path):\n",
    "    if (os.path.exists(path) is False):\n",
    "        return {}\n",
    "    f = open(path, 'rb')\n",
    "    result = pickle.load(f)\n",
    "    f.close()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "import statistics\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_trading_days = set(['2018-01-01', '2018-01-06', '2018-01-07', '2018-01-13', '2018-01-14', '2018-01-15', '2018-01-20', '2018-01-21', '2018-01-27', '2018-01-28', '2018-02-03', '2018-02-04', '2018-02-10', '2018-02-11', '2018-02-17', '2018-02-18', '2018-02-19', '2018-02-24', '2018-02-25', '2018-03-03', '2018-03-04', '2018-03-10', '2018-03-11', '2018-03-17', '2018-03-18', '2018-03-24', '2018-03-25', '2018-03-30', '2018-03-31', '2018-04-01', '2018-04-07', '2018-04-08', '2018-04-14', '2018-04-15', '2018-04-21', '2018-04-22', '2018-04-28', '2018-04-29', '2018-05-05', '2018-05-06', '2018-05-12', '2018-05-13', '2018-05-19', '2018-05-20', '2018-05-26', '2018-05-27', '2018-05-28', '2018-06-02', '2018-06-03', '2018-06-09', '2018-06-10', '2018-06-16', '2018-06-17', '2018-06-23', '2018-06-24', '2018-06-30', '2018-07-01', '2018-07-04', '2018-07-07', '2018-07-08', '2018-07-14', '2018-07-15', '2018-07-21', '2018-07-22', '2018-07-28', '2018-07-29', '2018-08-04', '2018-08-05', '2018-08-11', '2018-08-12', '2018-08-18', '2018-08-19', '2018-08-25', '2018-08-26', '2018-09-01', '2018-09-02', '2018-09-03', '2018-09-08', '2018-09-09', '2018-09-15', '2018-09-16', '2018-09-22', '2018-09-23', '2018-09-29', '2018-09-30', '2018-10-06', '2018-10-07', '2018-10-13', '2018-10-14', '2018-10-20', '2018-10-21', '2018-10-27', '2018-10-28', '2018-11-03', '2018-11-04', '2018-11-10', '2018-11-11', '2018-11-17', '2018-11-18', '2018-11-22', '2018-11-24', '2018-11-25', '2018-12-01', '2018-12-02', '2018-12-05', '2018-12-08', '2018-12-09', '2018-12-15', '2018-12-16', '2018-12-22', '2018-12-23', '2018-12-25', '2018-12-29', '2018-12-30', '2019-01-01', '2019-01-05', '2019-01-06', '2019-01-12', '2019-01-13', '2019-01-19', '2019-01-20', '2019-01-21', '2019-01-26', '2019-01-27', '2019-02-02', '2019-02-03', '2019-02-09', '2019-02-10', '2019-02-16', '2019-02-17', '2019-02-18', '2019-02-23', '2019-02-24', '2019-03-02', '2019-03-03', '2019-03-09', '2019-03-10', '2019-03-16', '2019-03-17', '2019-03-23', '2019-03-24', '2019-03-30', '2019-03-31', '2019-04-06', '2019-04-07', '2019-04-13', '2019-04-14', '2019-04-19', '2019-04-20', '2019-04-21', '2019-04-27', '2019-04-28', '2019-05-04', '2019-05-05', '2019-05-11', '2019-05-12', '2019-05-18', '2019-05-19', '2019-05-25', '2019-05-26', '2019-05-27', '2019-06-01', '2019-06-02', '2019-06-08', '2019-06-09', '2019-06-15', '2019-06-16', '2019-06-22', '2019-06-23', '2019-06-29', '2019-06-30', '2019-07-04', '2019-07-06', '2019-07-07', '2019-07-13', '2019-07-14', '2019-07-20', '2019-07-21', '2019-07-27', '2019-07-28', '2019-08-03', '2019-08-04', '2019-08-10', '2019-08-11', '2019-08-17', '2019-08-18', '2019-08-24', '2019-08-25', '2019-08-31', '2019-09-01', '2019-09-02', '2019-09-07', '2019-09-08', '2019-09-14', '2019-09-15', '2019-09-21', '2019-09-22', '2019-09-28', '2019-09-29', '2019-10-05', '2019-10-06', '2019-10-12', '2019-10-13', '2019-10-19', '2019-10-20', '2019-10-26', '2019-10-27', '2019-11-02', '2019-11-03', '2019-11-09', '2019-11-10', '2019-11-16', '2019-11-17', '2019-11-23', '2019-11-24', '2019-11-28', '2019-11-30', '2019-12-01', '2019-12-07', '2019-12-08', '2019-12-14', '2019-12-15', '2019-12-21', '2019-12-22', '2019-12-25', '2019-12-28', '2019-12-29', '2020-01-01', '2020-01-04', '2020-01-05', '2020-01-11', '2020-01-12', '2020-01-18', '2020-01-19', '2020-01-20', '2020-01-25', '2020-01-26', '2020-02-01', '2020-02-02', '2020-02-08', '2020-02-09', '2020-02-15', '2020-02-16', '2020-02-17', '2020-02-22', '2020-02-23', '2020-02-29', '2020-03-01', '2020-03-07', '2020-03-08', '2020-03-14', '2020-03-15', '2020-03-21', '2020-03-22', '2020-03-28', '2020-03-29', '2020-04-04', '2020-04-05', '2020-04-10', '2020-04-11', '2020-04-12', '2020-04-18', '2020-04-19', '2020-04-25', '2020-04-26', '2020-05-02', '2020-05-03', '2020-05-09', '2020-05-10', '2020-05-16', '2020-05-17', '2020-05-23', '2020-05-24', '2020-05-25', '2020-05-30', '2020-05-31', '2020-06-06', '2020-06-07', '2020-06-13', '2020-06-14', '2020-06-20', '2020-06-21', '2020-06-27', '2020-06-28', '2020-07-03', '2020-07-04', '2020-07-05', '2020-07-11', '2020-07-12', '2020-07-18', '2020-07-19', '2020-07-25', '2020-07-26', '2020-08-01', '2020-08-02', '2020-08-08', '2020-08-09', '2020-08-15', '2020-08-16', '2020-08-22', '2020-08-23', '2020-08-29', '2020-08-30', '2020-09-05', '2020-09-06', '2020-09-07', '2020-09-12', '2020-09-13', '2020-09-19', '2020-09-20', '2020-09-26', '2020-09-27', '2020-10-03', '2020-10-04', '2020-10-10', '2020-10-11', '2020-10-17', '2020-10-18', '2020-10-24', '2020-10-25', '2020-10-31', '2020-11-01', '2020-11-07', '2020-11-08', '2020-11-14', '2020-11-15', '2020-11-21', '2020-11-22', '2020-11-26', '2020-11-28', '2020-11-29'])\n",
    "\n",
    "def isTradingDay(time):\n",
    "    return '%d-%02d-%02d' % (time.year, time.month, time.day) not in not_trading_days\n",
    "\n",
    "def findCloseOpenCached(symbol, time, cached_prices):\n",
    "    day_increment = datetime.timedelta(days=1)\n",
    "\n",
    "    # Find first day if tweeted after 4pm\n",
    "    # If 4:00 on Wed, first day is Thursday\n",
    "    # If 4:00 on Friday, first day is Monday\n",
    "    if (time.hour >= 16):\n",
    "        time += day_increment\n",
    "\n",
    "    # If saturday, sunday or holiday, find first trading day to start from time\n",
    "    while (isTradingDay(time) == False):\n",
    "        time += day_increment\n",
    "\n",
    "    # Find next day based on the picked first day\n",
    "    end_day = time + day_increment\n",
    "    while (isTradingDay(end_day) == False):\n",
    "        end_day += day_increment\n",
    "\n",
    "    start_str = '%d-%02d-%02d' % (time.year, time.month, time.day)\n",
    "    end_str = '%d-%02d-%02d' % (end_day.year, end_day.month, end_day.day)\n",
    "    if (start_str not in cached_prices or end_str not in cached_prices or \n",
    "        symbol not in cached_prices[start_str] or \n",
    "        symbol not in cached_prices[end_str]):\n",
    "        return None\n",
    "\n",
    "    start = cached_prices[start_str][symbol]\n",
    "    end = cached_prices[end_str][symbol]\n",
    "    closePrice = start[1]\n",
    "    openPrice = end[0]\n",
    "    return (closePrice, openPrice, (openPrice - closePrice) * 100 / closePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_open = readPickleObject('newPickled/averaged.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedUserPrediction(user_values, symbol, feature_avg_std):\n",
    "    num_tweets = user_values['num_tweets']\n",
    "\n",
    "    # (1) Scale Tweet Number\n",
    "    max_value = feature_avg_std['num_tweets']['avg'] + (3 * feature_avg_std['num_tweets']['std'])\n",
    "    scaled_num_tweets = (num_tweets) / math.log10(max_value)\n",
    "    scaled_num_tweets = (scaled_num_tweets / 1.5) + 0.33\n",
    "\n",
    "    if (scaled_num_tweets > 1):\n",
    "        scaled_num_tweets = 1\n",
    "\n",
    "    # (2) Scale user returns\n",
    "    return_unique = user_values['return_unique']\n",
    "    return_unique_s = user_values['return_unique_s']\n",
    "\n",
    "    max_value = feature_avg_std['return_unique']['avg'] + (3 * feature_avg_std['return_unique']['std'])\n",
    "    scaled_return_unique = (math.log10(return_unique - 19)) / math.log10(max_value)\n",
    "    scaled_return_unique = (scaled_return_unique / 1.5) + 0.33\n",
    "    \n",
    "    max_value = feature_avg_std['return_unique_s']['avg'] + (3 * feature_avg_std['return_unique_s']['std'])\n",
    "    scaled_return_unique_s = (math.log10(return_unique_s - 4)) / math.log10(max_value)\n",
    "    scaled_return_unique_s = (scaled_return_unique_s / 1.5) + 0.33\n",
    "\n",
    "    if (scaled_return_unique > 1):\n",
    "        scaled_return_unique = 1\n",
    "    if (scaled_return_unique_s > 1):\n",
    "        scaled_return_unique_s = 1\n",
    "\n",
    "    # (3) all features combined (scale accuracy from 0.5 - 1 to between 0.7 - 1.2)\n",
    "    accuracy_unique = user_values['accuracy_unique'] + 0.3\n",
    "    all_features = accuracy_unique * scaled_num_tweets * scaled_return_unique\n",
    "    return (scaled_return_unique + (2 * scaled_num_tweets) + (1 * scaled_return_unique_s) + (2 * all_features)) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_sim(param):\n",
    "    stocks = ['SPY', 'TSLA', 'IBIO', 'AYTU', 'XSPA', 'GNUS', 'SPCE', 'INO', 'CODX', 'BA', 'AAPL', \n",
    "        'FCEL', 'AMD', 'SRNE', 'MARK', 'B', 'NIO', 'ONTX', 'ROKU', 'INPX', \n",
    "        'ACB', 'AMZN', 'SHLL', 'WKHS', 'BIOC', 'MVIS', 'DIS', 'VXRT', 'BYND', 'JNUG', \n",
    "        'TTOO', 'TVIX', 'TOPS', 'VTIQ', 'VBIV', 'TBLT', 'ADXS', 'AAL', 'CLVS', 'SHIP', 'GHSI', \n",
    "        'AMRN', 'UGAZ', 'AIM', 'ZOM', 'GILD', 'VISL', 'FB', 'HTBX', 'EROS', 'KTOV', 'HTZ', 'TTNP', \n",
    "        'TNXP', 'MSFT', 'ZM', 'UAVS', 'DGLY', 'QQQ', 'BNGO', 'NFLX', 'NVAX', 'MRNA', \n",
    "        'USO', 'MFA', 'IDEX', 'BB', 'RTTR', 'BABA', 'CCL', 'OPK', 'NOVN', 'SHOP', 'ENPH', 'BCRX', \n",
    "        'DK', 'SPEX', 'BYFC', 'OCGN', 'WTRH', 'AUPH', 'MNKD', 'FMCI', 'I', 'CORV', 'IZEA', \n",
    "        'NNVC', 'UBER', 'CEI', 'NCLH', 'NVDA', 'D', 'SQ', 'OPGN', 'NAK']\n",
    "\n",
    "    bucket = readPickleObject('newPickled/preprocessed_stock_user_features.pickle')\n",
    "    total_correct = 0\n",
    "    total_found = 0\n",
    "    days_back = 7\n",
    "    picked_dates = {}\n",
    "\n",
    "    # stocks=['ROKU']\n",
    "\n",
    "    for symbol in stocks:\n",
    "        local_correct = 0\n",
    "        local_total = 0\n",
    "        if (symbol not in bucket):\n",
    "            continue\n",
    "        dates = bucket[symbol]\n",
    "        avg_std_historical = {}\n",
    "        for date_str in dates:\n",
    "            day_res = dates[date_str]\n",
    "            avg_std = day_res['avg_std']\n",
    "            del bucket[symbol][date_str]['avg_std']\n",
    "            del bucket[symbol][date_str]['bull_count']\n",
    "            del bucket[symbol][date_str]['bear_count']\n",
    "\n",
    "            bull_w = 0\n",
    "            bull_count = 0\n",
    "            bear_w = 0\n",
    "            bear_count = 0\n",
    "            for user in dates[date_str]:\n",
    "                w = weightedUserPrediction(day_res[user], symbol, avg_std)\n",
    "                tweet_w = day_res[user]['w']\n",
    "                if (day_res[user]['prediction']):\n",
    "                    bull_w += (w * tweet_w)\n",
    "                    bull_count += 1\n",
    "                else:\n",
    "                    bear_w += (w * tweet_w)\n",
    "                    bear_count += 1\n",
    "\n",
    "            total_w = (1 * bull_w) - (1 * bear_w)\n",
    "            avg_std_historical[date_str] = {'bull_w': bull_w, 'bear_w': bear_w, 'total_w': total_w}\n",
    "            sorted_dates = list(avg_std_historical.keys())\n",
    "            if (len(sorted_dates) > days_back): # Remove if greater than max dates\n",
    "                first_date = sorted_dates[0]\n",
    "                del avg_std_historical[first_date]\n",
    "\n",
    "            curr_avg_std = {}\n",
    "            for date in avg_std_historical:\n",
    "                for f in avg_std_historical[date]:\n",
    "                    if (f not in curr_avg_std):\n",
    "                        curr_avg_std[f] = []\n",
    "                    curr_avg_std[f].append(avg_std_historical[date][f])\n",
    "\n",
    "            feature_avg_std = {}\n",
    "            for f in curr_avg_std: # Find avg/std for each feature over d days back\n",
    "                feature_avg_std[f] = {}\n",
    "                feature_avg_std[f]['avg'] = statistics.mean(curr_avg_std[f])\n",
    "                if (len(curr_avg_std[f]) == 1):\n",
    "                    feature_avg_std[f]['std'] = 1\n",
    "                    continue\n",
    "                feature_avg_std[f]['std'] = statistics.stdev(curr_avg_std[f])\n",
    "\n",
    "            closeopen = findCloseOpenCached(symbol, datetime.datetime.strptime(date_str, '%Y-%m-%d'), close_open)\n",
    "            if (closeopen == None):\n",
    "                continue\n",
    "            deviation = (total_w - feature_avg_std['total_w']['avg']) / feature_avg_std['total_w']['std']\n",
    "            # print(symbol, date_str, round(total_w, 2), deviation, closeopen[2])\n",
    "            if (deviation > 1.9):\n",
    "                if (closeopen[2] > 0):\n",
    "                    total_correct += 1\n",
    "                    local_correct += 1\n",
    "                total_found += 1\n",
    "                local_total += 1\n",
    "\n",
    "                if (date_str not in picked_dates):\n",
    "                    picked_dates[date_str] = []\n",
    "                picked_dates[date_str].append([symbol, deviation, closeopen[2]])\n",
    "            if (deviation < -1.9):\n",
    "                if (closeopen[2] < 0):\n",
    "                    total_correct += 1\n",
    "                    local_correct += 1\n",
    "                total_found += 1\n",
    "                local_total += 1\n",
    "                print(symbol, date_str, round(total_w, 2), deviation, closeopen[2])\n",
    "\n",
    "        # if (local_total > 0):\n",
    "        #     print(symbol, local_correct, local_total, local_correct / local_total, len(dates.keys()))\n",
    "        # else:\n",
    "        #     print(symbol, 'rip', len(dates.keys()))\n",
    "\n",
    "    actual_correct = 0\n",
    "    actual_total = 0\n",
    "    for date_str in sorted(picked_dates.keys()):\n",
    "        res = sorted(picked_dates[date_str], key=lambda x: x[1], reverse=True)\n",
    "        res = list(map(lambda x: [x[0], round(x[1], 2), round(x[2], 2)], res))\n",
    "        # print(date_str, res[:3])\n",
    "        for x in res[:3]:\n",
    "            if ((x[1] > 0 and x[2] > 0) or (x[1] < 0 and x[2] < 0)):\n",
    "                actual_correct += 1\n",
    "            actual_total += 1\n",
    "\n",
    "    return (total_correct, total_found, total_correct / total_found, actual_correct, actual_total, actual_correct / actual_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(233, 374, 0.6229946524064172, 158, 242, 0.6528925619834711)\n"
    }
   ],
   "source": [
    "\n",
    "print(run_sim(5))\n",
    "\n",
    "# for i in range(17, 22):\n",
    "#     res = run_sim(i)\n",
    "#     print(i, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bit8b405cec83804fbdaf717463c93f7e3a",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}